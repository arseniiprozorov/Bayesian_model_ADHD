# Bayesian Single Case Classification Model

The model detailed on this github page reflects specific assumptions that are built into the mathemetical foundations of the model. Users of this approach thus should be familiar with what the model is doing and how it works. While many more details are provided in the corresponding manuscript for the simulation study, a brief overview is provided here as well to accompany the github materials. Again, the primary aim of this resource is to help readers understand what the model is doing by clearly describing its assumptions. It is ultimately up to the clinician to utilize the model in a way that is appropriate.

## Modeling Assumptions

### Assumption 1: Diagnosis

The largest assumption of the model has to do with the way in which diagnosis is thought of. In essence, the model treats a diagnosis as a latent state that is manifest by producing varying distributions of test scores. While a large assumption, this is consistent with standard normative inferences. To demonstrate this, consider the use of a normative sample in standard clinical practice. Raw scores of the client are converted to standardized scores (e.g., scaled, standardized, T scores) using this normative reference sample, and if these standardized scores are substaintially below the average of the reference sample, then we begin to become concerned for the presence of impairment. Inference about whether a test performance is impaired is thus based on some level of consideration for whether scores are aberrantly low and, as such, would reflect a rarity for a person from that normative population. Implicit in this line of thinking then is that individuals from other populations (e.g., those with Alzheimer's dementia or cognitive sequelae from TBI as examples) might be more likely, or more frequently, produce scores around the same level as the client. The diagnostic assumption then of this model is that various diseases or disorders reflect unique populations definable by their own distributions of scores such that scores of certain levels are obtained more frequently or commonly in one poplation than the others. A diagnosis thus is given when the obtained scores more strongly align with, or are more commonly observed, in one of these specific populations.

### Assumption 2: Clinical Presentation

Related to the assumption of how a diagnosis can be made, the model also assumes something about how individuals present to assessments. By treating diangoses as unobserved/latent states that produce unique distributions of test scores, we need to think of how a single individual fits within this framework. The simplest way, and the way in which the model is built, is to treat a client as a random observation from some population. For example, if a client has Alzheimer's dementia, then the model is treating them as a single random observation from the population of individuals with Alzheimer's dementia. While we assume that individuals with Alzheimer's dementia will produce scores more similar to others with the same diagnosis than those with other diagnoses, we still should assume that there is variation in scores we observe in this population. The client thus may not produce a pathognomic Alzheimer's profile of scores, but instead, they will produce scores that, on average, are more typical of the Alzheimer's population. Since the person is a single random observation from this population, we expect that their scores will not be the exact average of the population but instead will have some level of sampling deviation that introduces some degree of uncertainty about the true diagnostic state of the individual.

### Assumption 3: Sufficient Statistics

So far, the modeling framework has been explained in the context of population distributions; however, this is rarely an obtainable distribution in research or clinical practice. Much more common, however, are samples of individuals with various cognitive disorders. Research studies, case series, and normative sample projects all commonly report some basic descriptive information about their samples that can be used to infer what the population parameters might be. Since the interest of this model are the populations' mean and covariance matrix, this means that we must find the sufficient statistics for estimating these parameters. Fortunately, approximation of the population distribution using sample means and standard deviations are well-defined in the statistical literature. Thus, the model assumes that the provided sample statistics (i.e., the means, standard deviations, and intercorrelation matrices) are sufficient for estimating the population parameters.

### Assumption 4: Multivariate Normality

Unfortunately, knowing the summary statistics of a reference sample is not enough to know the shape of the distribution itself. As a result, a parametric assumption is utilized to approximate the shape of the distribution in the population. Although a multivariate normal distribution is not a requirement for this general methodology, it is utilized in this iteration of the model because the described sample statistics are the sufficient statistics for describing the multivariate normal distribution. Additional sample information would be required for fitting alternative distributions like the binomial or skewed-normal. While a multivariate normal distributional assumption may not universally hold across populations or tests, we do think it is an appropriate simplification of the population parameters for many neuropsychological settings. A normal (or Gaussian) distribution will arise in nature when incremental sources of random error or variation contribute to the data source. As all neuropsychological tests are subject to random measurement error, we expect that most scores will form a reasonably normal distribution in the population. Similarly, cognitive ability is iteself subject to random variation through many competing direct and indirect differences in genetic and environmental factors. Thus, when tests include items over a sufficient range of difficulty, we anticipate that a normal distribution will emerge in the population of scores. This is not always observed in raw scores, but it is common for normative referencing to more closely bring the data toward a normal distribution. For this reason, we recommend that the model be used only with standardized scores.

### Assumption 5: A Population of Populations

The final assumption is a culmination of all the underlying assumptions already reviewed. Earlier we discussed the client as a random observation from their respective diagnostic population's distribution of scores; however, we also noted earlier that the diagnostic state of a client at assessment is latent and thus unobserved. Since the true population to which a client belongs is unknown at the time of assessment, the individual is instead a single random observation from a population of populations. Statistically, this is described as a mixture model. Literally, the population of all possible scores reflects a mixture of the distributions of scores from various populations, and the task of the diagnosis is thus to identify which of these sub-populations is the most likely population of origin for the single random observation the clinician is diagnosing. We thus try to learn what the client's latent diagnostic state (or class) truly is by comparing their performance on testing to known performance in reference samples (that are in turn telling us about their populations' distributions). The simple assumption here is that the population to whch a person belongs is also the population in which their test scores are most likely. Put simply, if someone has a T score of 80 on a memory measure, then we should be rightfully skeptical of an Alzheimer's diagnosis as such scores are so unlikely as to be impossible in that population.

## Modeling Visualization

The language above might be helpful for some users, but the idea of identifying latent classes and mixtures of populations may still be unfamiliar. To help clarify this point, we now provide a visual analogue to accompany the above and summarize what the model is ultimately trying to do. For this purpose, we consider only the univariate case (i.e., where we are examining just one test score) since adding additional dimensions quickly makes a noiser and harder to understand visual analogue. We also consider the case of just 3 latent populations/classes, but this is all readily scaled up to an arbitrary number of tests and populations. To being, consider a single test and the distribution of scores that we may observe for it if we were to administer it to everyone in the world. The following density plot is a hypothetical example of what we could observe in such a case:

![image](https://user-images.githubusercontent.com/76739193/133145086-56db105d-0cc1-4ae3-9b23-e655b720c77d.png)

The distribution appears to have a positive skew with most people doing average (around a T score of 50) with only some people doing better. At the same time, hardly anyone does worse than a 25 (or 2.5 standard deviations below the mean). At first glance, this distribution in the population might appear non-normal and thus not appropriate for the described model. If, however, we consider this distribution as instead reflecting a population of populations, then the story changes. Since this is hypothetical and simulated data, we know that there are 3 sub-populations within this larger distribution, so we now show the same distribution but with these 3 populations also shown:

![image](https://user-images.githubusercontent.com/76739193/133145126-9731573c-c54d-4885-b6ef-678313bedfa7.png)

As can be seen now, the overall frequency of scores are actually the result of combinations of frequencies from the 3 sub-populations. In this case, a total of 50,000 scores were simulated from 3 different populations: one with mean = 50 (10), another with mean = 40 (5), and the final with mean = 35 (2.5). Even though the 3 different sub-populations were sample from normal distributions are themselves quite normally distributed, the population of these populations is clearly skewed and deviates from this normal distribution. When mixtures of distributions, in this case mixtures of Gaussians, are present, the primary interest is in trying to identify the mixing ratio, or the proportion of cases that belong to each sub-population. In this case, we don't need to identify the mixing ratios because the data were simulated using known ratios. Approximately 30% of all cases are from the population with mean = 40, 50% from the population with mean = 50, and 20% from the population with mean = 35. Thus, without knowing anything about which sub-population each score belonged to, if we just guessed that any single score was from the one with a mean of 50, then we would be correct 50% of the time on average.

A clinician will obviously never just select whatever the most common diagnosis is in a particular setting and "guess" this for every single client whom is elevated by this clinician; instead, the clinician will collect data and then use this data to make an informed decision about the most likely diagnosis. Consider the case of two different scores, a T score of 50 and a T score of 38. These two scores are shown on the frequency distributions below as vertical dashed lines:

![image](https://user-images.githubusercontent.com/76739193/133146770-757d5ead-2122-4e65-a17a-35759cba39fa.png)

In the case where the obtained score is 50, it is clear that this score is most likely from the green sub-population (which had mean = 50 and sd = 10). The clinician can thus do better than just simply guessing the most common diagnosis and use this information to say that the scores are most consistent with the diagnostic class of the green sub-population. The result is less clear for the T score of 38. In this case, the most likely sub-population to which the score belongs is the red population (mean = 40, sd = 5) since a score of 38 is more frequent in that sub-population; however, a not insignificant number of people from the blue subpopulation (mean = 35, sd = 2.5) also get scores of 38. The clinician may ultimately conclude that the score is most likely from the red sub-population, but there is a reasonable chance that the score comes from blue sub-population and a smaller chance that it comes from the green sub-population. Knowing the mixing proportions, in this case, can help the clinician become more confident, however. Just based on these distributions, there is some ambiguity as to true membership but there is inofmration about likely or probable membership. Beyond just where the score falls in each distribution, we also know about how common scores from each of the sub-populations are, and we can combine this information ot derive a better idea as to what is the more likely class that a T score of 38 would belong to.

If we take the mixing ratios as our prior knowledge and just call it a pre-test probability, then we can use Bayes' rule to combine the pre-test probability with the likelihood of a 38 in each of the 3 sub-populations' distributions to get a post-test probability. What would be even better is if we knew distinguishing characteristics of each of the sub-populations. Right now, we are saying that in the whole population, these sub-populations occur with relative frequencies of 0.3, 0.5, and 0.2; however, say that red population is composed exclusively of individuals over the age of 65. If our T score of 38 came from a person who is 35, then we may become much more skeptical now of the red sub-population (i.e., our indiviualized pre-test probability shifts from 0.3 to something much closer to zero). In other words, even though one distribution seems like a much more obvious fit, we can utilize prior knowledge about our populations to make better guesses about the population that the score really belongs to.

To take this visual analogue back to the model at hand, the model is computing the likelihood of test scores across a range of sub-populations. The clinician is judiciously selecting candidate sub-populations to avoid falsely arriving at a bad diagnosis (e.g., a 25 year-old probably shouldn't be compared to an Alzheimer's dementia sample since that diagnosis would be very improbable at that age). At the same time, prior knowledge about the relative frequency of certain diagnoses (i.e., their mixing ratios) can be informatively passed to the model to compute the post-test probabilities of membership to various sub-populations. A "diagnosis" then comes from whichever sub-population's distribution make the scores most probable.
